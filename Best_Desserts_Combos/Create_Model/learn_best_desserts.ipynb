{
  "cells": [
    {
      "metadata": {
        "_uuid": "1faa04b7a078bdcc75b7f8ee88f74190ee7bf6ef"
      },
      "cell_type": "markdown",
      "source": "# **Dessert Flavor Combinations**\nThe goal here is predict what flavor combinations make for a good dessert. E.g. history tells us apple, cinammon, and nutmeg pair well because a large number of recipies use these flavors and these recipies are generally well rated. Here, I attempt to do this with a data set scrapped from epicurious.com. Basically, the features are a list of flavors present in the recipie and the target is the rating (0 to 4 stars)"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk as nltk\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "67f536930cd9abbce920d61347ceae3a35978efa"
      },
      "cell_type": "markdown",
      "source": "## 1) Load data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "dataset = pd.read_csv('../input/recipes.csv')\ndataset",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d63a10eef2208344b52fc120bbec7d95bf3f074"
      },
      "cell_type": "markdown",
      "source": "## 2) Clean Data"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "80feed509459d78af6e6b28d98bb4ca3a7c4d247"
      },
      "cell_type": "code",
      "source": "#Remove entires that dont have strings for the flavors\ndataset = dataset[dataset['flavors'].apply(lambda x: isinstance(x, str))]\n\n#tokenize the flavor string \ntemp = dataset['flavors'].apply(lambda x: np.unique(nltk.word_tokenize(x)))\n\n#Calc number of flavors\ndataset.loc[:,'num_flavors'] = temp.apply(lambda x: x.shape[0])\n\n#Remove entries with only 1 flavor\ndataset = dataset[dataset['num_flavors'] > 1]\n\n#convert rating to float\ndataset.loc[:,'rating'] = dataset['rating'].apply(lambda x: float(x))\n\ndataset.loc[:,'flavors_clean'] = temp.apply(lambda x: \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in x]).strip())\n\n#So, one thing I have yet to do is account for recipes that have no reviews...\n#I need to re-scrape the data set to get that info, for now lets just remove\n#recipies with a rating of 0\ndataset = dataset[dataset['rating'] != 0]\ndataset",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2672e4ff1567d85a94f15772abd189f4c40a4418"
      },
      "cell_type": "markdown",
      "source": "## 3) Preprocess"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd99079d82164f1fd5f893a36892121cef6a6456"
      },
      "cell_type": "code",
      "source": "#Get features and targets\nfeatures = dataset['flavors_clean'].values\ntargets = dataset['rating'].values\n\n#First lets try the hold-out method for validation\ntrain, test, y_train, y_test = train_test_split(features, targets, test_size=0.40, random_state=0)\ntest, valid, y_test, y_valid = train_test_split(test, y_test, test_size=0.40, random_state=0)\n\nprint(train.shape)\nprint(test.shape)\nprint(valid.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb6c5b68b941fa55ad7045f00d94953fb58df17e"
      },
      "cell_type": "code",
      "source": "#Set up a pipeline:\n#   1) Use TF-IDF to generate features\n#   2) maybe other things later????\nvectorizer = make_pipeline(\n    TfidfVectorizer(sublinear_tf=True)\n)\n\nx_train = vectorizer.fit_transform(train)\nx_test = vectorizer.transform(test)\nx_valid = vectorizer.transform(valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78310e877a0e4e3c53f15cf546d99b8f01af10e6"
      },
      "cell_type": "markdown",
      "source": "## 4) Check out our data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8acfe03baadf3a22df198d17cba151cfb43d1099"
      },
      "cell_type": "code",
      "source": "#Start with dimensional reduction to view it in 2D\ndim_red = TruncatedSVD(2)\nreduced_data = dim_red.fit_transform(x_train)\nreduced_data.shape\nplt.scatter(reduced_data[:,0],reduced_data[:,1])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f183532647f72808d19ace6bfba64e816500c01b"
      },
      "cell_type": "markdown",
      "source": "## 5) Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17ab1941b0d486219befa6209fa545286cbb08af"
      },
      "cell_type": "code",
      "source": "#model = SVR(\n#    C=50,\n#    kernel='linear',\n#    gamma=1.4,\n#    coef0=1,\n#    cache_size=100,\n#)\n\nmodel = RandomForestRegressor(\n    n_estimators = 4\n)\n\n#model = GradientBoostingRegressor(\n#)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fad9cb6294cb1e2a385264e3277742beb68ba958"
      },
      "cell_type": "markdown",
      "source": "## 6) Train"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4321e1e434c0c752da48b4b2e6775afcaa171c74"
      },
      "cell_type": "code",
      "source": "model.fit(x_train,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "656ac68c3346469cc0e318a388f30a32e6b50b82"
      },
      "cell_type": "markdown",
      "source": "## 7) Validate"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1606cd961922c619d6990b59845c76eec28c095b"
      },
      "cell_type": "code",
      "source": "y_pred_valid = model.predict(x_valid)\nmean_absolute_error(y_valid, y_pred_valid)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}